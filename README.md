# AI-Image-Generation-using-Diffusion-Models


Diffusion Models are a class of **generative models** that create new data (such as images) by starting from pure noise and gradually removing it step by step â€” guided by a text prompt or conditioning input.

This project demonstrates how to use **Diffusion Models** for AI image generation using **Python**. Youâ€™ll learn the basic concepts, the underlying process, and how to implement and run a simple diffusion-based generator.

---

## ğŸš€ Features

* Generate images from text prompts
* Understand how diffusion processes denoise random noise into structured images
* Explore pretrained models from libraries like **Hugging Face Diffusers**
* Experiment with parameters (steps, guidance scale, etc.) for creative control

---

## ğŸ§© How It Works

1. **Start with noise** â€” a random tensor that looks like static TV noise.
2. **Iteratively denoise** â€” the model predicts and removes a portion of the noise at each step.
3. **Generate image** â€” after several iterations, a clear image emerges that matches the input prompt.

---

## ğŸ§° Requirements

Make sure you have the following installed:

```bash
pip install torch torchvision diffusers transformers accelerate
```

---

## ğŸ§‘â€ğŸ’» Usage

Run the main script:

```bash
python generate_image.py --prompt "A scenic view of Madina in watercolor painting style"
```

Example output:

```
ğŸ–¼ï¸ Generating image for prompt: "A scenic view of Madina in watercolor painting style"
âœ… Image saved as output.png
```

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ generate_image.py     # Main script for generating images
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ README.md             # Project documentation
â””â”€â”€ outputs/              # Generated images
`
If you like this project, â­ star it on GitHub and share your feedback!
